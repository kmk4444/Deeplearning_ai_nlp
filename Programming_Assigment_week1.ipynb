{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMoLiskKxcYvzhne3UpXbFf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kmk4444/Deeplearning_ai_nlp/blob/main/Programming_Assigment_week1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment 1: Logistic Regression\n",
        "Welcome to week one of this specialization. You will learn about logistic regression. Concretely, you will be implementing logistic regression for sentiment analysis on tweets. Given a tweet, you will decide if it has a positive sentiment or a negative one. Specifically you will:\n",
        "\n",
        "* Learn how to extract features for logistic regression given some text\n",
        "* Implement logistic regression from scratch\n",
        "* Apply logistic regression on a natural language processing task\n",
        "* Test using your logistic regression\n",
        "* Perform error analysis\n"
      ],
      "metadata": {
        "id": "gFXV43Z_ZcGH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Table of Contents\n",
        "\n",
        "- [Import Functions and Data](#0)\n",
        "- [1 - Logistic Regression](#1)\n",
        "    - [1.1 - Sigmoid](#1-1)\n",
        "        - [Exercise 1 - sigmoid (UNQ_C1)](#ex-1)\n",
        "    - [1.2 - Cost function and Gradient](#1-2)\n",
        "        - [Exercise 2 - gradientDescent (UNQ_C2)](#ex-2)\n",
        "- [2 - Extracting the Features](#2)\n",
        "    - [Exercise 3 - extract_features (UNQ_C3)](#ex-3)\n",
        "- [3 - Training Your Model](#3)\n",
        "- [4 - Test your Logistic Regression](#4)\n",
        "    - [Exercise 4 - predict_tweet (UNQ_C4)](#ex-4)\n",
        "    - [4.1 - Check the Performance using the Test Set](#4-1)\n",
        "        - [Exercise 5 - test_logistic_regression (UNQ_C5)](#ex-5)\n",
        "- [5 - Error Analysis](#5)\n",
        "- [6 - Predict with your own Tweet](#6)"
      ],
      "metadata": {
        "id": "E6Yoa-pteRfj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name='0'></a>\n",
        "## Import Functions and Data"
      ],
      "metadata": {
        "id": "TjH4P1y8eUMU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ovES1D9aT3zG"
      },
      "outputs": [],
      "source": [
        "# run this cell to import nltk\n",
        "import nltk\n",
        "from os import getcwd\n",
        "\n",
        "nltk.download('twitter_samples')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Imported Functions\n",
        "\n",
        "Download the data needed for this assignment. Check out the [documentation for the twitter_samples dataset](http://www.nltk.org/howto/twitter.html).\n",
        "\n",
        "* twitter_samples: if you're running this notebook on your local computer, you will need to download it using:\n",
        "```Python\n",
        "nltk.download('twitter_samples')\n",
        "```\n",
        "\n",
        "* stopwords: if you're running this notebook on your local computer, you will need to download it using:\n",
        "```python\n",
        "nltk.download('stopwords')\n",
        "```\n",
        "\n",
        "#### Import some helper functions that we provided in the utils.py file:\n",
        "* process_tweet: cleans the text, tokenizes it into separate words, removes stopwords, and converts words to stems.\n",
        "* build_freqs: this counts how often a word in the 'corpus' (the entire set of tweets) was associated with a positive label '1' or a negative label '0', then builds the 'freqs' dictionary, where each key is the (word,label) tuple, and the value is the count of its frequency within the corpus of tweets."
      ],
      "metadata": {
        "id": "SvucVed9ezHU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filePath = f\"{getcwd()}/../tmp2/\"\n",
        "nltk.data.path.append(filePath)"
      ],
      "metadata": {
        "id": "OXeg3IWiez6r"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import string\n",
        "import numpy as np\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import TweetTokenizer"
      ],
      "metadata": {
        "id": "ccFHjuDmhvDy"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_tweet(tweet):\n",
        "    \"\"\"Process tweet function.\n",
        "    Input:\n",
        "        tweet: a string containing a tweet\n",
        "    Output:\n",
        "        tweets_clean: a list of words containing the processed tweet\n",
        "\n",
        "    \"\"\"\n",
        "    stemmer = PorterStemmer()\n",
        "    stopwords_english = stopwords.words('english')\n",
        "    # remove stock market tickers like $GE\n",
        "    tweet = re.sub(r'\\$\\w*', '', tweet)\n",
        "    # remove old style retweet text \"RT\"\n",
        "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
        "    # remove hyperlinks\n",
        "    tweet = re.sub(r'https?://[^\\s\\n\\r]+', '', tweet)\n",
        "    # remove hashtags\n",
        "    # only removing the hash # sign from the word\n",
        "    tweet = re.sub(r'#', '', tweet)\n",
        "    # tokenize tweets\n",
        "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n",
        "                               reduce_len=True)\n",
        "    tweet_tokens = tokenizer.tokenize(tweet)\n",
        "\n",
        "    tweets_clean = []\n",
        "    for word in tweet_tokens:\n",
        "        if (word not in stopwords_english and  # remove stopwords\n",
        "                word not in string.punctuation):  # remove punctuation\n",
        "            # tweets_clean.append(word)\n",
        "            stem_word = stemmer.stem(word)  # stemming word\n",
        "            tweets_clean.append(stem_word)\n",
        "\n",
        "    return tweets_clean"
      ],
      "metadata": {
        "id": "b5MsjcD9fCuH"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_freqs(tweets, ys):\n",
        "    \"\"\"Build frequencies.\n",
        "    Input:\n",
        "        tweets: a list of tweets\n",
        "        ys: an m x 1 array with the sentiment label of each tweet\n",
        "            (either 0 or 1)\n",
        "    Output:\n",
        "        freqs: a dictionary mapping each (word, sentiment) pair to its\n",
        "        frequency\n",
        "    \"\"\"\n",
        "    # Convert np array to list since zip needs an iterable.\n",
        "    # The squeeze is necessary or the list ends up with one element.\n",
        "    # Also note that this is just a NOP if ys is already a list.\n",
        "    yslist = np.squeeze(ys).tolist()\n",
        "\n",
        "    # Start with an empty dictionary and populate it by looping over all tweets\n",
        "    # and over all processed words in each tweet.\n",
        "    freqs = {}\n",
        "    for y, tweet in zip(yslist, tweets):\n",
        "        for word in process_tweet(tweet):\n",
        "            pair = (word, y)\n",
        "            if pair in freqs:\n",
        "                freqs[pair] += 1\n",
        "            else:\n",
        "                freqs[pair] = 1\n",
        "\n",
        "    return freqs"
      ],
      "metadata": {
        "id": "ivSlOWl-fDz9"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from nltk.corpus import twitter_samples"
      ],
      "metadata": {
        "id": "jZgURhype_i1"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prepare the Data\n",
        "* The `twitter_samples` contains subsets of five thousand positive_tweets, five thousand negative_tweets, and the full set of 10,000 tweets.  \n",
        "    * If you used all three datasets, we would introduce duplicates of the positive tweets and negative tweets.  \n",
        "    * You will select just the five thousand positive tweets and five thousand negative tweets."
      ],
      "metadata": {
        "id": "lDJcOrbyfLbV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# select the set of positive and negative tweets\n",
        "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
        "all_negative_tweets = twitter_samples.strings('negative_tweets.json')"
      ],
      "metadata": {
        "id": "qHGlTRakfMK2"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Train test split: 20% will be in the test set, and 80% in the training set.\n"
      ],
      "metadata": {
        "id": "CaJon4jpfoUG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# split the data into two pieces, one for training and one for testing (validation set)\n",
        "test_pos = all_positive_tweets[4000:]\n",
        "train_pos = all_positive_tweets[:4000]\n",
        "test_neg = all_negative_tweets[4000:]\n",
        "train_neg = all_negative_tweets[:4000]\n",
        "\n",
        "train_x = train_pos + train_neg\n",
        "test_X = test_pos + test_neg"
      ],
      "metadata": {
        "id": "PqYP2-ctfoqK"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Create the numpy array of positive labels and negative labels."
      ],
      "metadata": {
        "id": "7ES9i8jgf_Yo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# combine positive and negative labels\n",
        "train_y = np.append(np.ones((len(train_pos), 1)), np.zeros((len(train_neg), 1)), axis=0)\n",
        "test_y = np.append(np.ones((len(test_pos), 1)), np.zeros((len(test_neg), 1)), axis=0)"
      ],
      "metadata": {
        "id": "juCDM2aHgAE5"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_y)"
      ],
      "metadata": {
        "id": "vL9QGK8SgMYv",
        "outputId": "4d2fd83d-bee9-47b2-d267-326350f757cf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1.]\n",
            " [1.]\n",
            " [1.]\n",
            " ...\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the shape train and test sets\n",
        "print(\"train_y.shape = \" + str(train_y.shape))\n",
        "print(\"test_y.shape = \" + str(test_y.shape))"
      ],
      "metadata": {
        "id": "19SC6HtGgs6L",
        "outputId": "09275c1d-9792-4ce4-e8c7-dc2832f76dfe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_y.shape = (8000, 1)\n",
            "test_y.shape = (2000, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create frequency dictionary\n",
        "freqs = build_freqs(train_x, train_y)"
      ],
      "metadata": {
        "id": "is2B7zaShgUq"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check the output\n",
        "print(\"type(freqs) = \" + str(type(freqs)))\n",
        "print(\"len(freqs) = \" + str(len(freqs.keys())))"
      ],
      "metadata": {
        "id": "014QVtpjh2m5",
        "outputId": "3ddfad01-1533-4691-9398-4fdb53f7abbd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "type(freqs) = <class 'dict'>\n",
            "len(freqs) = 11427\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Process Tweet\n",
        "The given function 'process_tweet' tokenizes the tweet into individual words, removes stop words and applies stemming."
      ],
      "metadata": {
        "id": "dVQwBLyaiLFj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# test the function below\n",
        "print(\"This is an example of positive tweet: \\n\", train_x[0])\n",
        "print(\"\\nThis is an example of the processed version of the tweet: \\n\",process_tweet(train_x[0]))"
      ],
      "metadata": {
        "id": "wThjtH6MiQxP",
        "outputId": "6daf6d98-62d7-458b-f32e-3075c5d45279",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is an example of positive tweet: \n",
            " #FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)\n",
            "\n",
            "This is an example of the processed version of the tweet: \n",
            " ['followfriday', 'top', 'engag', 'member', 'commun', 'week', ':)']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name='1'></a>\n",
        "## 1 - Logistic Regression\n",
        "\n",
        "<a name='1-1'></a>\n",
        "### 1.1 - Sigmoid\n",
        "You will learn to use logistic regression for text classification.\n",
        "* The sigmoid function is defined as:\n",
        "\n",
        "$$ h(z) = \\frac{1}{1+\\exp^{-z}} \\tag{1}$$\n",
        "\n",
        "It maps the input 'z' to a value that ranges between 0 and 1, and so it can be treated as a probability.\n",
        "\n",
        "<div style=\"width:image width px; font-size:100%; text-align:center;\"><img src='https://github.com/kmk4444/Deeplearning_ai_nlp/blob/main/images/sigmoid_plot.jpg?raw=1' alt=\"alternate text\" width=\"width\" height=\"height\" style=\"width:300px;height:200px;\" /> Figure 1 </div>"
      ],
      "metadata": {
        "id": "nFhi4I4PirwP"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "H9U2dWt1iu1D"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}