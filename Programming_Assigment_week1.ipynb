{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO66NhQSnFPBJWyGeHJB4/U",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kmk4444/Deeplearning_ai_nlp/blob/main/Programming_Assigment_week1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment 1: Logistic Regression\n",
        "Welcome to week one of this specialization. You will learn about logistic regression. Concretely, you will be implementing logistic regression for sentiment analysis on tweets. Given a tweet, you will decide if it has a positive sentiment or a negative one. Specifically you will:\n",
        "\n",
        "* Learn how to extract features for logistic regression given some text\n",
        "* Implement logistic regression from scratch\n",
        "* Apply logistic regression on a natural language processing task\n",
        "* Test using your logistic regression\n",
        "* Perform error analysis\n"
      ],
      "metadata": {
        "id": "gFXV43Z_ZcGH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Table of Contents\n",
        "\n",
        "- [Import Functions and Data](#0)\n",
        "- [1 - Logistic Regression](#1)\n",
        "    - [1.1 - Sigmoid](#1-1)\n",
        "        - [Exercise 1 - sigmoid (UNQ_C1)](#ex-1)\n",
        "    - [1.2 - Cost function and Gradient](#1-2)\n",
        "        - [Exercise 2 - gradientDescent (UNQ_C2)](#ex-2)\n",
        "- [2 - Extracting the Features](#2)\n",
        "    - [Exercise 3 - extract_features (UNQ_C3)](#ex-3)\n",
        "- [3 - Training Your Model](#3)\n",
        "- [4 - Test your Logistic Regression](#4)\n",
        "    - [Exercise 4 - predict_tweet (UNQ_C4)](#ex-4)\n",
        "    - [4.1 - Check the Performance using the Test Set](#4-1)\n",
        "        - [Exercise 5 - test_logistic_regression (UNQ_C5)](#ex-5)\n",
        "- [5 - Error Analysis](#5)\n",
        "- [6 - Predict with your own Tweet](#6)"
      ],
      "metadata": {
        "id": "E6Yoa-pteRfj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name='0'></a>\n",
        "## Import Functions and Data"
      ],
      "metadata": {
        "id": "TjH4P1y8eUMU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ovES1D9aT3zG"
      },
      "outputs": [],
      "source": [
        "# run this cell to import nltk\n",
        "import nltk\n",
        "from os import getcwd\n",
        "\n",
        "nltk.download('twitter_samples')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Imported Functions\n",
        "\n",
        "Download the data needed for this assignment. Check out the [documentation for the twitter_samples dataset](http://www.nltk.org/howto/twitter.html).\n",
        "\n",
        "* twitter_samples: if you're running this notebook on your local computer, you will need to download it using:\n",
        "```Python\n",
        "nltk.download('twitter_samples')\n",
        "```\n",
        "\n",
        "* stopwords: if you're running this notebook on your local computer, you will need to download it using:\n",
        "```python\n",
        "nltk.download('stopwords')\n",
        "```\n",
        "\n",
        "#### Import some helper functions that we provided in the utils.py file:\n",
        "* process_tweet: cleans the text, tokenizes it into separate words, removes stopwords, and converts words to stems.\n",
        "* build_freqs: this counts how often a word in the 'corpus' (the entire set of tweets) was associated with a positive label '1' or a negative label '0', then builds the 'freqs' dictionary, where each key is the (word,label) tuple, and the value is the count of its frequency within the corpus of tweets."
      ],
      "metadata": {
        "id": "SvucVed9ezHU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filePath = f\"{getcwd()}/../tmp2/\"\n",
        "nltk.data.path.append(filePath)"
      ],
      "metadata": {
        "id": "OXeg3IWiez6r"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import string\n",
        "import numpy as np\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import TweetTokenizer"
      ],
      "metadata": {
        "id": "ccFHjuDmhvDy"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_tweet(tweet):\n",
        "    \"\"\"Process tweet function.\n",
        "    Input:\n",
        "        tweet: a string containing a tweet\n",
        "    Output:\n",
        "        tweets_clean: a list of words containing the processed tweet\n",
        "\n",
        "    \"\"\"\n",
        "    stemmer = PorterStemmer()\n",
        "    stopwords_english = stopwords.words('english')\n",
        "    # remove stock market tickers like $GE\n",
        "    tweet = re.sub(r'\\$\\w*', '', tweet)\n",
        "    # remove old style retweet text \"RT\"\n",
        "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
        "    # remove hyperlinks\n",
        "    tweet = re.sub(r'https?://[^\\s\\n\\r]+', '', tweet)\n",
        "    # remove hashtags\n",
        "    # only removing the hash # sign from the word\n",
        "    tweet = re.sub(r'#', '', tweet)\n",
        "    # tokenize tweets\n",
        "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n",
        "                               reduce_len=True)\n",
        "    tweet_tokens = tokenizer.tokenize(tweet)\n",
        "\n",
        "    tweets_clean = []\n",
        "    for word in tweet_tokens:\n",
        "        if (word not in stopwords_english and  # remove stopwords\n",
        "                word not in string.punctuation):  # remove punctuation\n",
        "            # tweets_clean.append(word)\n",
        "            stem_word = stemmer.stem(word)  # stemming word\n",
        "            tweets_clean.append(stem_word)\n",
        "\n",
        "    return tweets_clean"
      ],
      "metadata": {
        "id": "b5MsjcD9fCuH"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_freqs(tweets, ys):\n",
        "    \"\"\"Build frequencies.\n",
        "    Input:\n",
        "        tweets: a list of tweets\n",
        "        ys: an m x 1 array with the sentiment label of each tweet\n",
        "            (either 0 or 1)\n",
        "    Output:\n",
        "        freqs: a dictionary mapping each (word, sentiment) pair to its\n",
        "        frequency\n",
        "    \"\"\"\n",
        "    # Convert np array to list since zip needs an iterable.\n",
        "    # The squeeze is necessary or the list ends up with one element.\n",
        "    # Also note that this is just a NOP if ys is already a list.\n",
        "    yslist = np.squeeze(ys).tolist()\n",
        "\n",
        "    # Start with an empty dictionary and populate it by looping over all tweets\n",
        "    # and over all processed words in each tweet.\n",
        "    freqs = {}\n",
        "    for y, tweet in zip(yslist, tweets):\n",
        "        for word in process_tweet(tweet):\n",
        "            pair = (word, y)\n",
        "            if pair in freqs:\n",
        "                freqs[pair] += 1\n",
        "            else:\n",
        "                freqs[pair] = 1\n",
        "\n",
        "    return freqs"
      ],
      "metadata": {
        "id": "ivSlOWl-fDz9"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from nltk.corpus import twitter_samples"
      ],
      "metadata": {
        "id": "jZgURhype_i1"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prepare the Data\n",
        "* The `twitter_samples` contains subsets of five thousand positive_tweets, five thousand negative_tweets, and the full set of 10,000 tweets.  \n",
        "    * If you used all three datasets, we would introduce duplicates of the positive tweets and negative tweets.  \n",
        "    * You will select just the five thousand positive tweets and five thousand negative tweets."
      ],
      "metadata": {
        "id": "lDJcOrbyfLbV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# select the set of positive and negative tweets\n",
        "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
        "all_negative_tweets = twitter_samples.strings('negative_tweets.json')"
      ],
      "metadata": {
        "id": "qHGlTRakfMK2"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Train test split: 20% will be in the test set, and 80% in the training set.\n"
      ],
      "metadata": {
        "id": "CaJon4jpfoUG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# split the data into two pieces, one for training and one for testing (validation set)\n",
        "test_pos = all_positive_tweets[4000:]\n",
        "train_pos = all_positive_tweets[:4000]\n",
        "test_neg = all_negative_tweets[4000:]\n",
        "train_neg = all_negative_tweets[:4000]\n",
        "\n",
        "train_x = train_pos + train_neg\n",
        "test_X = test_pos + test_neg"
      ],
      "metadata": {
        "id": "PqYP2-ctfoqK"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Create the numpy array of positive labels and negative labels."
      ],
      "metadata": {
        "id": "7ES9i8jgf_Yo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# combine positive and negative labels\n",
        "train_y = np.append(np.ones((len(train_pos), 1)), np.zeros((len(train_neg), 1)), axis=0)\n",
        "test_y = np.append(np.ones((len(test_pos), 1)), np.zeros((len(test_neg), 1)), axis=0)"
      ],
      "metadata": {
        "id": "juCDM2aHgAE5"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vL9QGK8SgMYv",
        "outputId": "4d2fd83d-bee9-47b2-d267-326350f757cf"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1.]\n",
            " [1.]\n",
            " [1.]\n",
            " ...\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the shape train and test sets\n",
        "print(\"train_y.shape = \" + str(train_y.shape))\n",
        "print(\"test_y.shape = \" + str(test_y.shape))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "19SC6HtGgs6L",
        "outputId": "09275c1d-9792-4ce4-e8c7-dc2832f76dfe"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_y.shape = (8000, 1)\n",
            "test_y.shape = (2000, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create frequency dictionary\n",
        "freqs = build_freqs(train_x, train_y)"
      ],
      "metadata": {
        "id": "is2B7zaShgUq"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check the output\n",
        "print(\"type(freqs) = \" + str(type(freqs)))\n",
        "print(\"len(freqs) = \" + str(len(freqs.keys())))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "014QVtpjh2m5",
        "outputId": "3ddfad01-1533-4691-9398-4fdb53f7abbd"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "type(freqs) = <class 'dict'>\n",
            "len(freqs) = 11427\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Process Tweet\n",
        "The given function 'process_tweet' tokenizes the tweet into individual words, removes stop words and applies stemming."
      ],
      "metadata": {
        "id": "dVQwBLyaiLFj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# test the function below\n",
        "print(\"This is an example of positive tweet: \\n\", train_x[0])\n",
        "print(\"\\nThis is an example of the processed version of the tweet: \\n\",process_tweet(train_x[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wThjtH6MiQxP",
        "outputId": "6daf6d98-62d7-458b-f32e-3075c5d45279"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is an example of positive tweet: \n",
            " #FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)\n",
            "\n",
            "This is an example of the processed version of the tweet: \n",
            " ['followfriday', 'top', 'engag', 'member', 'commun', 'week', ':)']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name='1'></a>\n",
        "## 1 - Logistic Regression\n",
        "\n",
        "<a name='1-1'></a>\n",
        "### 1.1 - Sigmoid\n",
        "You will learn to use logistic regression for text classification.\n",
        "* The sigmoid function is defined as:\n",
        "\n",
        "$$ h(z) = \\frac{1}{1+\\exp^{-z}} \\tag{1}$$\n",
        "\n",
        "It maps the input 'z' to a value that ranges between 0 and 1, and so it can be treated as a probability.\n",
        "\n",
        "<div style=\"width:image width px; font-size:100%; text-align:center;\"><img src='https://github.com/kmk4444/Deeplearning_ai_nlp/blob/main/images/sigmoid_plot.jpg?raw=1' alt=\"alternate text\" width=\"width\" height=\"height\" style=\"width:300px;height:200px;\" /> Figure 1 </div>"
      ],
      "metadata": {
        "id": "nFhi4I4PirwP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name='ex-1'></a>\n",
        "### Exercise 1 -  sigmoid\n",
        "Implement the sigmoid function.\n",
        "* You will want this function to work if z is a scalar as well as if it is an array."
      ],
      "metadata": {
        "id": "jGCVhSWsi7YR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details>    \n",
        "<summary>\n",
        "    <font size=\"3\" color=\"darkgreen\"><b>Hints</b></font>\n",
        "</summary>\n",
        "<p>\n",
        "<ul>\n",
        "    <li><a href=\"https://docs.scipy.org/doc/numpy/reference/generated/numpy.exp.html\" > numpy.exp </a> </li>\n",
        "\n",
        "</ul>\n",
        "</p>\n",
        "\n"
      ],
      "metadata": {
        "id": "LBk-0PkMi-6j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# UNQ_C1 GRADED FUNCTION: sigmoid\n",
        "def sigmoid(z):\n",
        "    '''\n",
        "    Input:\n",
        "        z: is the input (can be a scalar or an array)\n",
        "        Output:\n",
        "        h: the sigmoid of z\n",
        "    '''\n",
        "\n",
        "    ### START CODE HERE ###\n",
        "    # calculate the sigmoid of z\n",
        "    h = 1 / (1 + np.exp(-z))\n",
        "        ### END CODE HERE ###\n",
        "\n",
        "    return h\n"
      ],
      "metadata": {
        "id": "gk0nyluPi_UL"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing your function\n",
        "if (sigmoid(0) == 0.5):\n",
        "    print('SUCCESS!')\n",
        "else:\n",
        "    print('Oops!')\n",
        "\n",
        "if (sigmoid(4.92) == 0.9927537604041685):\n",
        "    print('CORRECT!')\n",
        "else:\n",
        "    print('Oops again!')"
      ],
      "metadata": {
        "id": "N91QD9mSke8v",
        "outputId": "b893a09d-5833-4fab-fc79-03ca7fd9ee43",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SUCCESS!\n",
            "CORRECT!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Logistic Regression: Regression and a Sigmoid\n",
        "\n",
        "Logistic regression takes a regular linear regression, and applies a sigmoid to the output of the linear regression.\n",
        "\n",
        "Regression:\n",
        "$$z = \\theta_0 x_0 + \\theta_1 x_1 + \\theta_2 x_2 + ... \\theta_N x_N$$\n",
        "Note that the $\\theta$ values are \"weights\". If you took the deep learning specialization, we referred to the weights with the 'w' vector.  In this course, we're using a different variable $\\theta$ to refer to the weights.\n",
        "\n",
        "Logistic regression\n",
        "$$ h(z) = \\frac{1}{1+\\exp^{-z}}$$\n",
        "$$z = \\theta_0 x_0 + \\theta_1 x_1 + \\theta_2 x_2 + ... \\theta_N x_N$$\n",
        "We will refer to 'z' as the 'logits'."
      ],
      "metadata": {
        "id": "DIVc1UZfkvPO"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_4q31MdBkwAf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name='1-2'></a>\n",
        "### 1.2 - Cost function and Gradient\n",
        "\n",
        "The cost function used for logistic regression is the average of the log loss across all training examples:\n",
        "\n",
        "$$J(\\theta) = -\\frac{1}{m} \\sum_{i=1}^m y^{(i)}\\log (h(z(\\theta)^{(i)})) + (1-y^{(i)})\\log (1-h(z(\\theta)^{(i)}))\\tag{5} $$\n",
        "* $m$ is the number of training examples\n",
        "* $y^{(i)}$ is the actual label of training example 'i'.\n",
        "* $h(z^{(i)})$ is the model's prediction for the training example 'i'.\n",
        "\n",
        "The loss function for a single training example is\n",
        "$$ Loss = -1 \\times \\left( y^{(i)}\\log (h(z(\\theta)^{(i)})) + (1-y^{(i)})\\log (1-h(z(\\theta)^{(i)})) \\right)$$\n",
        "\n",
        "* All the $h$ values are between 0 and 1, so the logs will be negative. That is the reason for the factor of -1 applied to the sum of the two loss terms.\n",
        "* Note that when the model predicts 1 ($h(z(\\theta)) = 1$) and the label 'y' is also 1, the loss for that training example is 0.\n",
        "* Similarly, when the model predicts 0 ($h(z(\\theta)) = 0$) and the actual label is also 0, the loss for that training example is 0.\n",
        "* However, when the model prediction is close to 1 ($h(z(\\theta)) = 0.9999$) and the label is 0, the second term of the log loss becomes a large negative number, which is then multiplied by the overall factor of -1 to convert it to a positive loss value. $-1 \\times (1 - 0) \\times log(1 - 0.9999) \\approx 9.2$ The closer the model prediction gets to 1, the larger the loss."
      ],
      "metadata": {
        "id": "RKxSYMFHlAgj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# verify that when the model predicts close to 1, but the actual label is 0, the loss is a large positive value\n",
        "-1 * (1 - 0) * np.log(1 - 0.9999) # loss is about 9.2"
      ],
      "metadata": {
        "id": "VU4ai0HIlBmx",
        "outputId": "d1fccbc0-b8d7-4d90-b456-f4504f68f091",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9.210340371976294"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Likewise, if the model predicts close to 0 ($h(z) = 0.0001$) but the actual label is 1, the first term in the loss function becomes a large number: $-1 \\times log(0.0001) \\approx 9.2$.  The closer the prediction is to zero, the larger the loss."
      ],
      "metadata": {
        "id": "7JAKTKBNlrTq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# verify that when the model predicts close to 0 but the actual label is 1, the loss is a large positive value\n",
        "-1 * np.log(0.0001) # loss is about 9.2"
      ],
      "metadata": {
        "id": "QY6ynefLlsUT",
        "outputId": "8c4d6f7f-fcb2-4e6b-f5ab-45b7470461d3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9.210340371976182"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    }
  ]
}